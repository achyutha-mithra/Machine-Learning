{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yI07bbhr2TNM"
   },
   "source": [
    "Loading all the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "TUZz-Gidnu45",
    "outputId": "591c3fcd-e3fa-4e0e-edf6-ba837165dbc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "from string import digits\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qZyMg5YB2YTp"
   },
   "source": [
    "The dataset, which has two paragraphs per sample and the objective here is to find similarity between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "njl4vPLCnu5D"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"gdrive/My Drive/datasets/Text_Similarity_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "K4n1SUsDnu5H",
    "outputId": "93caaf2a-72da-40e2-da52-fbaf92d80059"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>savvy searchers fail to spot ads internet sear...</td>\n",
       "      <td>newcastle 2-1 bolton kieron dyer smashed home ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>millions to miss out on the net by 2025  40% o...</td>\n",
       "      <td>nasdaq planning $100m share sale the owner of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>young debut cut short by ginepri fifteen-year-...</td>\n",
       "      <td>ruddock backs yapp s credentials wales coach m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>diageo to buy us wine firm diageo  the world s...</td>\n",
       "      <td>mci shares climb on takeover bid shares in us ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>be careful how you code a new european directi...</td>\n",
       "      <td>media gadgets get moving pocket-sized devices ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>india seeks to boost construction india has cl...</td>\n",
       "      <td>music mogul fuller sells company pop idol supr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>podcasters  look to net money nasa is doing it...</td>\n",
       "      <td>ukip outspent labour on eu poll the uk indepen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>row over  police  power for csos the police fe...</td>\n",
       "      <td>ban on hunting comes into force fox hunting wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>election  could be terror target  terrorists m...</td>\n",
       "      <td>nhs waiting time target is cut hospital waitin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>japan economy slides to recession the japanese...</td>\n",
       "      <td>optimism remains over uk housing the uk proper...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique_ID  ...                                              text2\n",
       "0          0  ...  newcastle 2-1 bolton kieron dyer smashed home ...\n",
       "1          1  ...  nasdaq planning $100m share sale the owner of ...\n",
       "2          2  ...  ruddock backs yapp s credentials wales coach m...\n",
       "3          3  ...  mci shares climb on takeover bid shares in us ...\n",
       "4          4  ...  media gadgets get moving pocket-sized devices ...\n",
       "5          5  ...  music mogul fuller sells company pop idol supr...\n",
       "6          6  ...  ukip outspent labour on eu poll the uk indepen...\n",
       "7          7  ...  ban on hunting comes into force fox hunting wi...\n",
       "8          8  ...  nhs waiting time target is cut hospital waitin...\n",
       "9          9  ...  optimism remains over uk housing the uk proper...\n",
       "\n",
       "[10 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "28oh6RB1nu5P"
   },
   "outputs": [],
   "source": [
    "text1 = data.text1.values\n",
    "text2 = data.text2.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OuCSnlq52jI1"
   },
   "source": [
    "Preprocessing the paragraphs. Very basic: will remove\n",
    "* digits \n",
    "* special characters \n",
    "* checks for punctuation \n",
    "* removes stop words.\n",
    "\n",
    "***Also,  lets not lemmatize or stem the words. stemmed version of the words may be missing in vocabulary (GloVe)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HtSPsiej_wbI"
   },
   "outputs": [],
   "source": [
    "def process_text(txt):    \n",
    "    for i in range(len(text1)):\n",
    "\n",
    "        txt[i] = re.sub(r'\\W+', ' ', txt[i])\n",
    "        remove_digits = str.maketrans('', '', digits)\n",
    "        txt[i] = txt[i].translate(remove_digits)\n",
    "        txt[i] = txt[i].translate(str.maketrans('', '', string.punctuation))\n",
    "        txt[i] = re.sub(r'\\b\\w{1,2}\\b', '', txt[i])\n",
    "        txt[i] = [i for i in txt[i].lower().split() if i not in stop]\n",
    "    return txt\n",
    "\n",
    "text1 = process_text(text1)\n",
    "text2 = process_text(text2)\n",
    "\n",
    "data.text1 = text1\n",
    "data.text2 = text2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "colab_type": "code",
    "id": "811KH3OEnu5Z",
    "outputId": "ad2d215e-4e01-4bbb-9282-2a60e51f310b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[savvy, searchers, fail, spot, ads, internet, ...</td>\n",
       "      <td>[newcastle, bolton, kieron, dyer, smashed, hom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[millions, miss, net, population, still, witho...</td>\n",
       "      <td>[nasdaq, planning, share, sale, owner, technol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[young, debut, cut, short, ginepri, fifteen, y...</td>\n",
       "      <td>[ruddock, backs, yapp, credentials, wales, coa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[diageo, buy, wine, firm, diageo, world, bigge...</td>\n",
       "      <td>[mci, shares, climb, takeover, bid, shares, ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[careful, code, new, european, directive, coul...</td>\n",
       "      <td>[media, gadgets, get, moving, pocket, sized, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>[india, seeks, boost, construction, india, cle...</td>\n",
       "      <td>[music, mogul, fuller, sells, company, pop, id...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>[podcasters, look, net, money, nasa, year, old...</td>\n",
       "      <td>[ukip, outspent, labour, poll, independence, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>[row, police, power, csos, police, federation,...</td>\n",
       "      <td>[ban, hunting, comes, force, fox, hunting, dog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>[election, could, terror, target, terrorists, ...</td>\n",
       "      <td>[nhs, waiting, time, target, cut, hospital, wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>[japan, economy, slides, recession, japanese, ...</td>\n",
       "      <td>[optimism, remains, housing, property, market,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique_ID  ...                                              text2\n",
       "0          0  ...  [newcastle, bolton, kieron, dyer, smashed, hom...\n",
       "1          1  ...  [nasdaq, planning, share, sale, owner, technol...\n",
       "2          2  ...  [ruddock, backs, yapp, credentials, wales, coa...\n",
       "3          3  ...  [mci, shares, climb, takeover, bid, shares, ph...\n",
       "4          4  ...  [media, gadgets, get, moving, pocket, sized, d...\n",
       "5          5  ...  [music, mogul, fuller, sells, company, pop, id...\n",
       "6          6  ...  [ukip, outspent, labour, poll, independence, p...\n",
       "7          7  ...  [ban, hunting, comes, force, fox, hunting, dog...\n",
       "8          8  ...  [nhs, waiting, time, target, cut, hospital, wa...\n",
       "9          9  ...  [optimism, remains, housing, property, market,...\n",
       "\n",
       "[10 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0tGgu_ss3fAN"
   },
   "source": [
    "Lets see which are the most occuring words in our corpus. This function returns the words and their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AT_31vpX3pV3"
   },
   "outputs": [],
   "source": [
    "def count_most_common(col):\n",
    "  full_list = []  \n",
    "  for elmnt in data[col]:  \n",
    "      full_list += elmnt\n",
    "  val_counts = pd.Series(full_list).value_counts() \n",
    "  return val_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oe5rEghJ3x2Y"
   },
   "source": [
    "Most repeated words of our first column (text1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "Nmu3GTSfnu5g",
    "outputId": "2a986568-3038-4709-c1c0-aad70ad86fc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said      13072\n",
      "would      4657\n",
      "year       4192\n",
      "also       3924\n",
      "people     3693\n",
      "new        3589\n",
      "one        3454\n",
      "could      2731\n",
      "last       2498\n",
      "first      2486\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "val_counts_text1 = count_most_common(\"text1\")\n",
    "print(val_counts_text1[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gsq8vdqa35Pe"
   },
   "source": [
    "Most repeated words of our second column (text1):\n",
    "\n",
    "***looking at the common words alone, these two columns or rather the collection of paragraphs definitely have commonalities.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "440kKKSXnu5k",
    "outputId": "2d903eab-1c69-46a0-a890-b8125653aefe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "said      13041\n",
      "would      4680\n",
      "year       4162\n",
      "also       3883\n",
      "people     3665\n",
      "new        3590\n",
      "one        3454\n",
      "could      2735\n",
      "last       2532\n",
      "first      2454\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "val_counts_text2 = count_most_common(\"text2\")\n",
    "print(val_counts_text2[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7unzYYLA4N6B"
   },
   "source": [
    "Lets get the two columns in array format, to be worked upon next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wmq6I-7inu5p"
   },
   "outputs": [],
   "source": [
    "text1 = data.text1.values.tolist()\n",
    "text2 = data.text2.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "j6ApB3V1nu5y",
    "outputId": "7f837929-4b4a-40a5-9ef7-514c07095607"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4023, 4023)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text1), len(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "72wFXpBc4x99"
   },
   "source": [
    "Lets join the two lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a018FGCj4sgU"
   },
   "outputs": [],
   "source": [
    "joined_list = text1 + text2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NKSf1P0l4pG0"
   },
   "source": [
    "* Going forward, I tried using LSTM network for the tokenized data. Results were very bad. LSTM network did not learn due to many possible reasons, one of which is that the data has straightforward correlation. \n",
    "***To avoid that, lets introduce Noice.**\n",
    "\n",
    "<ol>\n",
    "  <li> t1_freq and t2_freq columns will have the lengths of individual lengths per sample. </li>\n",
    "  <li> t1_t2_intersect as the name suggests has the intersection(common words) between the two paragraphs per sample.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bXd93Cewnu51"
   },
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "d_dict = defaultdict(set)\n",
    "for i in range(data.shape[0]):\n",
    "  d_dict[\" \".join(data.text1[i])].add(\" \".join(data.text2[i]))\n",
    "  d_dict[\" \".join(data.text2[i])].add(\" \".join(data.text1[i]))\n",
    "\n",
    "\n",
    "def t1_freq(row):\n",
    "    return(len(row['text1']))\n",
    "    \n",
    "def t2_freq(row):\n",
    "    return(len(row['text2']))\n",
    "    \n",
    "def t1_t2_intersect(row):\n",
    "    # print(row['text1'])\n",
    "    return len(list(set(row['text1']) & set(row['text2'])))\n",
    "\n",
    "\n",
    "data['t1_t2_intersect'] = data.apply(t1_t2_intersect, axis=1, raw=True)\n",
    "data['t1_freq'] = data.apply(t1_freq, axis=1, raw=True)\n",
    "data['t2_freq'] = data.apply(t2_freq, axis=1, raw=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "id": "sPDFk_cQbXqO",
    "outputId": "647ea447-2cd2-4a3e-ad3f-6de710692b41"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>t1_t2_intersect</th>\n",
       "      <th>t1_freq</th>\n",
       "      <th>t2_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[savvy, searchers, fail, spot, ads, internet, ...</td>\n",
       "      <td>[newcastle, bolton, kieron, dyer, smashed, hom...</td>\n",
       "      <td>13</td>\n",
       "      <td>266</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[millions, miss, net, population, still, witho...</td>\n",
       "      <td>[nasdaq, planning, share, sale, owner, technol...</td>\n",
       "      <td>8</td>\n",
       "      <td>276</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[young, debut, cut, short, ginepri, fifteen, y...</td>\n",
       "      <td>[ruddock, backs, yapp, credentials, wales, coa...</td>\n",
       "      <td>11</td>\n",
       "      <td>181</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[diageo, buy, wine, firm, diageo, world, bigge...</td>\n",
       "      <td>[mci, shares, climb, takeover, bid, shares, ph...</td>\n",
       "      <td>12</td>\n",
       "      <td>89</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[careful, code, new, european, directive, coul...</td>\n",
       "      <td>[media, gadgets, get, moving, pocket, sized, d...</td>\n",
       "      <td>44</td>\n",
       "      <td>534</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4018</th>\n",
       "      <td>4018</td>\n",
       "      <td>[labour, plans, maternity, pay, rise, maternit...</td>\n",
       "      <td>[seasonal, lift, house, market, swathe, figure...</td>\n",
       "      <td>14</td>\n",
       "      <td>253</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4019</th>\n",
       "      <td>4019</td>\n",
       "      <td>[high, fuel, costs, hit, airlines, two, larges...</td>\n",
       "      <td>[new, media, battle, bafta, awards, bbc, leads...</td>\n",
       "      <td>7</td>\n",
       "      <td>173</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4020</th>\n",
       "      <td>4020</td>\n",
       "      <td>[britons, growing, digitally, obese, gadget, l...</td>\n",
       "      <td>[film, star, fox, behind, theatre, bid, leadin...</td>\n",
       "      <td>19</td>\n",
       "      <td>295</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4021</th>\n",
       "      <td>4021</td>\n",
       "      <td>[holmes, hit, hamstring, injury, kelly, holmes...</td>\n",
       "      <td>[tsunami, hit, sri, lanka, banks, sri, lanka, ...</td>\n",
       "      <td>5</td>\n",
       "      <td>120</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4022</th>\n",
       "      <td>4022</td>\n",
       "      <td>[nuclear, dumpsite, plan, attacked, plans, all...</td>\n",
       "      <td>[factor, show, gets, second, series, talent, s...</td>\n",
       "      <td>4</td>\n",
       "      <td>205</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4023 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unique_ID  ... t2_freq\n",
       "0             0  ...     362\n",
       "1             1  ...     109\n",
       "2             2  ...     202\n",
       "3             3  ...     187\n",
       "4             4  ...     393\n",
       "...         ...  ...     ...\n",
       "4018       4018  ...     250\n",
       "4019       4019  ...     156\n",
       "4020       4020  ...     237\n",
       "4021       4021  ...     128\n",
       "4022       4022  ...     111\n",
       "\n",
       "[4023 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2tSBBf-o6IKK"
   },
   "source": [
    "Lets scale our noice related data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7pNAiakKfUTZ"
   },
   "outputs": [],
   "source": [
    "noice = data[['t1_t2_intersect', 't1_freq', 't2_freq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yMartX2xft7R"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "ss = StandardScaler()\n",
    "\n",
    "ss.fit(noice)\n",
    "noice = ss.transform(noice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "FRIv2Q8rnu5u",
    "outputId": "ad73467b-c44f-4243-8940-5eeff6ba1ed8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['factor', 'show'],\n",
       " ['savvy', 'searchers'],\n",
       " ['newcastle', 'bolton'],\n",
       " ['nuclear', 'dumpsite'])"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_list[-1][:2],joined_list[0][:2],joined_list[4023][:2],joined_list[4022][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M1rwsms26Wnw"
   },
   "source": [
    "tokenize function returns the tokenized padded sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 96
    },
    "colab_type": "code",
    "id": "V1LK4FoYnu54",
    "outputId": "4f5233d3-fd41-4c41-9acd-e9b8d8c3ae14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 27545\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def tokenize(num, list_arr):\n",
    "    num = num #max size of library\n",
    "    name_tokenizer = Tokenizer(num_words=num)\n",
    "    name_tokenizer.fit_on_texts(list(list_arr))\n",
    "    name_tokenized_list = name_tokenizer.texts_to_sequences(list(list_arr))\n",
    "    \n",
    "    if len(name_tokenizer.word_index) < num:\n",
    "        num = len(name_tokenizer.word_index)\n",
    "      \n",
    "    print('Number of words:', num)\n",
    "    pad_name = len(max(name_tokenized_list,key=len))\n",
    "    final_array = pad_sequences(name_tokenized_list, maxlen=pad_name, padding='post')\n",
    "\n",
    "    return num, name_tokenizer, name_tokenized_list, pad_name, final_array\n",
    "\n",
    "max_num_word_text, tokenizer_text, list_tokenized_text, pad_max_text, text_array = tokenize(100000, joined_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ObbE-sft6ixG"
   },
   "source": [
    "Lets use GloVe 300 Dimensions word embedding weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "URsAPlrtxblw",
    "outputId": "2858e82a-61e3-4c15-c563-75f59a883ff9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-05 09:54:30--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2020-02-05 09:54:30--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2020-02-05 09:54:31--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  1.99MB/s    in 6m 29s  \n",
      "\n",
      "2020-02-05 10:01:01 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "lQ6nzEMFy9_U",
    "outputId": "e97f8aee-fd83-4759-a385-4c53708a57a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "!unzip glove*.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ClakZ3apAXPO",
    "outputId": "faa0e50c-2575-4b65-aacb-0f4dca6f32fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot stat 'glove.6B.300d.txt': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!mv \"glove.6B.300d.txt\" \"gdrive/My Drive/datasets/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zZo_ZOAy6p1q"
   },
   "source": [
    "The one with embedding dimention 300 is read in from the text file as a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4mMchPhGy0GK",
    "outputId": "7e3c731a-76e1-4585-acbe-fa5d21b7ef1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('gdrive/My Drive/datasets/glove.6B.300d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fuEqe_3n7NBr"
   },
   "source": [
    "Lets create an embedding matrix. According to the pre-trained word vector, the embedding matrix is billt. Words not found is marked all-zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7lrlyWjTziRE"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((max_num_word_text + 1, 300))\n",
    "for word, i in tokenizer_text.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sK4lKxwm7ZsC"
   },
   "source": [
    "Lets divide the tokenized version of entire corpus (joined list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aFTvoklZnu58",
    "outputId": "89dd7cd4-291e-42b0-9d96-f33dde425cbe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4023, 4023)"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_array1 = text_array[:4023]\n",
    "text_array2 = text_array[4023:]\n",
    "\n",
    "len(text_array1), len(text_array2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16PLG65K7gEo"
   },
   "source": [
    "Splitting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vu7a9iE4pMIj"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_array1_train,text_array1_test,text_array2_train,text_array2_test,noice_train,noice_test,text1_train,text1_test,text2_train,text2_test=train_test_split(text_array1,text_array2,noice,text1,text2,test_size=0.3, shuffle=True, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fxH1zBEl7o2r"
   },
   "source": [
    "# The approach hereon:\n",
    "<ol>\n",
    "<li> For the training data, lets calculate a very straightforward manhattan distance labels (using scipy's spatial). These will be our labels. Lets call them dummy labels.\n",
    "<li> Our model will try to learn from these labels and possibly learn to give better similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ERhwzUT95S6U"
   },
   "outputs": [],
   "source": [
    "def manhattan_distance_wordembedding_method(s1, s2):\n",
    "    import scipy\n",
    "    vector_1 = np.mean([embeddings_index.get(word) for word in s1 if word in embeddings_index],axis=0)\n",
    "    vector_2 = np.mean([embeddings_index.get(word) for word in s2 if word in embeddings_index],axis=0)\n",
    "    cityblock = scipy.spatial.distance.cityblock(vector_1, vector_2)\n",
    "    return round((1-cityblock),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZQR-FR4CPUV"
   },
   "outputs": [],
   "source": [
    "\n",
    "dummy_targets = []\n",
    "for i in range(len(text1_train)):\n",
    "  dummy_targets.append(manhattan_distance_wordembedding_method(text1_train[i],text2_train[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "15WbB2qx8Twd"
   },
   "source": [
    "Lets normalize our dummy targets (The scores were all negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "9f3_qAxI6rjw",
    "outputId": "75e059ab-bf3c-42c7-b639-dedf0a6b34dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7791289 , 0.72319257, 0.44417422, 0.29206279, 0.6743312 ,\n",
       "       0.41101039, 0.54123369, 0.43643599, 0.50210038, 0.65774928,\n",
       "       0.40128233, 0.43842582, 0.52907362, 0.48640283, 0.47932788,\n",
       "       0.5693124 , 0.67322574, 0.54012823, 0.60225514, 0.63431351,\n",
       "       0.45699757, 0.56776476, 0.66393986, 0.42073845, 0.56157418,\n",
       "       0.52221977, 0.48817157, 0.6460314 , 0.56245855, 0.42383374])"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_targets = dummy_targets/min(dummy_targets)\n",
    "dummy_targets[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FGEkIkE6_ePJ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.python.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vu4kZFHd8jhl"
   },
   "source": [
    "This is a Siamese LSTM model, wherein:\n",
    "<ol>\n",
    "<li> inputs share the embedding weights </li>\n",
    "<li> output of embedding layer also share a common LSTM layer (weights once again shared </li>\n",
    "<li> Noice values will have a separate Dense layer. </li>\n",
    "<li> outputs of shared-LSTM and Desne are then concatenated. </li>\n",
    "<li> Batch normalization in between for better performance and speed. </li>\n",
    "<li> Final Dense layer with sigmoid Activation (0-1 similarity score) </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ahEhwpkcnu6h"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Concatenate, Dense, Dropout, Flatten, LSTM, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Reshape, Dot\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adadelta, Adam, RMSprop, Adagrad\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import Add, Activation, Lambda, Conv1D, MaxPool1D, concatenate\n",
    "from keras.layers import Bidirectional, SpatialDropout1D, CuDNNLSTM\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "def similarity_score():\n",
    "\n",
    "    embedding_layer = Embedding(max_num_word_text + 1,\n",
    "                                   300,\n",
    "                                   weights=[embedding_matrix],\n",
    "                                   input_length=pad_max_text,\n",
    "                                   trainable=False)\n",
    "    \n",
    "    left_input = Input(shape=(pad_max_text,), dtype='int32')\n",
    "    right_input = Input(shape=(pad_max_text,), dtype='int32')\n",
    "    \n",
    "    encoded_left = embedding_layer(left_input)\n",
    "    encoded_right = embedding_layer(right_input)\n",
    "    \n",
    "    shared_lstm = CuDNNLSTM(200)\n",
    "    \n",
    "    left_output = shared_lstm(encoded_left)\n",
    "    right_output = shared_lstm(encoded_right)\n",
    "\n",
    "    noice_input = Input(shape=(noice.shape[1],))\n",
    "    noice_dense = Dense(50, activation=\"relu\")(noice_input)\n",
    "\n",
    "    merged = concatenate([left_output, right_output, noice_dense])\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dropout(0.4)(merged)\n",
    "\n",
    "    merged = Dense(100, activation=\"relu\")(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dropout(0.4)(merged)\n",
    "\n",
    "    preds = Dense(1, activation='sigmoid')(merged)\n",
    "    \n",
    "    model = Model(inputs=[left_input, right_input, noice_input], outputs=preds)\n",
    "    opt =Adam()\n",
    "    model.compile(loss='binary_crossentropy', optimizer=\"nadam\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 655
    },
    "colab_type": "code",
    "id": "Bs25qkHjOP5K",
    "outputId": "f8e4c811-1ecd-4601-bb89-1381dc4d08ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           (None, 2137)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           (None, 2137)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 2137, 300)    8263800     input_19[0][0]                   \n",
      "                                                                 input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_7 (CuDNNLSTM)        (None, 200)          401600      embedding_7[0][0]                \n",
      "                                                                 embedding_7[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 50)           200         input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 450)          0           cu_dnnlstm_7[0][0]               \n",
      "                                                                 cu_dnnlstm_7[1][0]               \n",
      "                                                                 dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 450)          1800        concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 450)          0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 100)          45100       dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 100)          400         dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 100)          0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 1)            101         dropout_14[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 8,713,001\n",
      "Trainable params: 448,101\n",
      "Non-trainable params: 8,264,900\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = similarity_score()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "colab_type": "code",
    "id": "K9cTl0YE_74p",
    "outputId": "fb39bbc2-b540-445b-e799-33f5b14b9e27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "2816/2816 [==============================] - 16s 6ms/step - loss: 0.8264\n",
      "Epoch 2/12\n",
      "2816/2816 [==============================] - 15s 5ms/step - loss: 0.7600\n",
      "Epoch 3/12\n",
      "2816/2816 [==============================] - 15s 5ms/step - loss: 0.7241\n",
      "Epoch 4/12\n",
      "2816/2816 [==============================] - 15s 5ms/step - loss: 0.7064\n",
      "Epoch 5/12\n",
      "2816/2816 [==============================] - 15s 5ms/step - loss: 0.6964\n",
      "Epoch 6/12\n",
      "2816/2816 [==============================] - 15s 5ms/step - loss: 0.6887\n",
      "Epoch 7/12\n",
      "2816/2816 [==============================] - 15s 5ms/step - loss: 0.6846\n",
      "Epoch 8/12\n",
      "2816/2816 [==============================] - 15s 5ms/step - loss: 0.6832\n",
      "Epoch 9/12\n",
      "2816/2816 [==============================] - 15s 5ms/step - loss: 0.6815\n",
      "Epoch 10/12\n",
      "2816/2816 [==============================] - 15s 5ms/step - loss: 0.6817\n",
      "Epoch 11/12\n",
      "2816/2816 [==============================] - 15s 5ms/step - loss: 0.6812\n",
      "Epoch 12/12\n",
      "2816/2816 [==============================] - 15s 5ms/step - loss: 0.6812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9918847400>"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = [text_array1_train, text_array2_train,noice_train], y =dummy_targets, batch_size=64, epochs=12,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N8uhdvWXAdtu"
   },
   "source": [
    "Lets predict for the entire dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sfLnwqpprQPK"
   },
   "outputs": [],
   "source": [
    "preds = model.predict([text_array1,text_array2,noice])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nlie0M0uAh4u"
   },
   "source": [
    "Lets round scores upto 2nd Decimal place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_z7a9gjokEpk",
    "outputId": "30bd6054-d3b2-4c04-fca9-c3d3aca98d83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.54, 0.57, 0.53, 0.49, 0.33, 0.49, 0.57, 0.41, 0.48, 0.36]"
      ]
     },
     "execution_count": 255,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "flat_list = [item for sublist in preds for item in sublist]\n",
    "\n",
    "flat_list = [math.ceil(i*100.0)/100.0 for i in flat_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FSTqbIBlA91x"
   },
   "source": [
    "Final Submission dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5dzhbY5slfFa"
   },
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(\n",
    "    {'Unique_ID': data.iloc[:,0],\n",
    "     'Similarity_Score': flat_list\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "id": "AJVD3EBOHPmx",
    "outputId": "b7948b2c-70a9-4781-8532-f876e1100eaf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>Similarity_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4018</th>\n",
       "      <td>4018</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4019</th>\n",
       "      <td>4019</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4020</th>\n",
       "      <td>4020</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4021</th>\n",
       "      <td>4021</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4022</th>\n",
       "      <td>4022</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4023 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unique_ID  Similarity_Score\n",
       "0             0              0.54\n",
       "1             1              0.57\n",
       "2             2              0.53\n",
       "3             3              0.49\n",
       "4             4              0.33\n",
       "...         ...               ...\n",
       "4018       4018              0.51\n",
       "4019       4019              0.58\n",
       "4020       4020              0.46\n",
       "4021       4021              0.60\n",
       "4022       4022              0.61\n",
       "\n",
       "[4023 rows x 2 columns]"
      ]
     },
     "execution_count": 271,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d-QcdtpPv5Tv"
   },
   "outputs": [],
   "source": [
    "final_df.to_csv('gdrive/My Drive/datasets/bhuvSubmission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Text Similarity.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
