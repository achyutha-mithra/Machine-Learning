{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kaggle1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzADcKz-MQV-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data_train = pd.read_csv('gdrive/My Drive/train.csv')\n",
        "data_test = pd.read_csv('gdrive/My Drive/test.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjQefBkWMStl",
        "colab_type": "code",
        "outputId": "e36f7d82-575d-4fc6-ab62-b4c1320cbd6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "data_train.head(4)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword  ...                                               text target\n",
              "0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
              "1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
              "2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
              "3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
              "\n",
              "[4 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX2DDRrOlMD7",
        "colab_type": "text"
      },
      "source": [
        "Checking whether the dataset is imbalanced. \n",
        "Thankfully, its not. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbyjgM57MUhG",
        "colab_type": "code",
        "outputId": "e6b0d8cb-7d8b-48ee-9d36-b3c520c3ecdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from collections import Counter\n",
        "Counter(data_train.target.values)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 4342, 1: 3271})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDN8ScYSPg0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = data_train[[\"id\",\"target\"]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiSR91o7PiUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train = data_train.drop([\"target\"],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eici98DtlYbG",
        "colab_type": "text"
      },
      "source": [
        "Lets combine train and test datasets so that preprocessing can be done simultaneously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttp9F6IFPkdM",
        "colab_type": "code",
        "outputId": "b714d141-a40f-4ccd-a8ea-8d6ca2a2f648",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data = pd.concat([data_train,data_test],axis=0)\n",
        "len(data) == len(data_test) + len(data_train)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPHjeBq2Ply-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = data.text.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKQrY-4pl-dV",
        "colab_type": "text"
      },
      "source": [
        "Lets clean some tweets. \n",
        "\n",
        "\n",
        "*   What I noticed was some tweets had links, which can be removed.\n",
        "*   Some tweets had text like \"x89Û_\" and so on. \n",
        "*   Removed usernames text followed by @\n",
        "*   Removed stop words.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqFAcsTOPsRL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "02a86088-d435-4720-ccd8-6675f07afc5e"
      },
      "source": [
        "import nltk\n",
        "import string\n",
        "from string import digits\n",
        "import re\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop = set(stopwords.words('english'))\n",
        "for i in range(len(text)):\n",
        "    \n",
        "    text[i] = re.sub(r'http\\S+', '', text[i])\n",
        "    text[i] = re.sub(r'https\\S+', '', text[i])\n",
        "    text[i] = re.sub(\"\\n\",\"\",text[i])\n",
        "    text[i] = re.sub(\":\",\"\",text[i])\n",
        "    text[i] = re.sub(\"&gt\",\"\",text[i])\n",
        "    text[i] = re.sub('@[^\\s]+','',text[i])\n",
        "    text[i] = re.sub(\"-\",\"\",text[i]) #see\n",
        "    text[i] = re.sub(\"crash&gt\",\"crash\",text[i])\n",
        "    text[i] = re.sub(\"tee\\x89Û_\",\"\",text[i])\n",
        "    \n",
        "    text[i] = re.sub(\"CarolinaåÊAblaze\",\"Carolina Ablaze\",text[i])\n",
        "    text[i] = re.sub(\"\\nme\",\"\",text[i])\n",
        "    text[i] = re.sub(\"\\nmom\",\"\",text[i])\n",
        "    text[i] = re.sub(\"\\n\\nIn\",\"\",text[i])\n",
        "    text[i] = re.sub(\"GtxRWm\",\"\",text[i])\n",
        "    text[i] = re.sub(\"yycstorm\",\"\",text[i])\n",
        "    text[i] = re.sub(\"&amp\",\"\",text[i])\n",
        "    text[i] = re.sub(\"CityofCalgary\",\"Calgary\",text[i])\n",
        "    text[i] = re.sub(\"\\x89ã¢\",\"\",text[i])\n",
        "    text[i] = re.sub(\"\\x89Û_\",\"\",text[i])\n",
        "    text[i] = re.sub(\"\\x89ÛÒ\",\"\",text[i])\n",
        "    text[i] = re.sub(\"XrWn\",\"\",text[i])\n",
        "    text[i] = re.sub(\"NOOOOOOOOO\",\"\",text[i])\n",
        "    text[i] = re.sub(\"Ayyo dei.\",\"\",text[i])\n",
        "    text[i] = re.sub(\"stormchase\",\"Storm\",text[i])\n",
        "    text[i] = re.sub(\"#AR #NC #OK\",\"#Arizona #New York #Oklahoma\",text[i])\n",
        "    text[i] = re.sub(\"BathAndNorthEastSomerset\",\"North east Somerset\",text[i])\n",
        "    text[i] = re.sub('tnwx', '', text[i])\n",
        "    text[i] = re.sub(\"\\x89ÛÏ#HannaPH\\x89Û\\x9d\",\"\",text[i])\n",
        "    text[i] = re.sub(\"JapÌ_n\",\"Japan\",text[i])\n",
        "    \n",
        "    \n",
        "    text[i] = re.sub(\"goooooooaaaaaal\",\"goal\",text[i])\n",
        "    text[i] = re.sub(\"#explodingkittens\\x89Û_ https://t.co/TFGrAyuDC5\",\"\",text[i] )\n",
        "    text[i] = re.sub(\"#explodingkittens\\x89Û_ https://t.co/TFGrAyuDC5\",\"\",text[i] )\n",
        "    text[i] = re.sub(\"gameofkittens\",\"game of kittens\",text[i])\n",
        "    text[i] = re.sub(\"looooool\",\"lol\",text[i])\n",
        "    \n",
        "    text[i] = re.sub(\"SOOOO\",\"so\",text[i])\n",
        "    text[i] = re.sub(r'\\W+', ' ', text[i])\n",
        "    text[i] = re.sub(\"TampaBay\",\"Tampa Bay\", text[i])\n",
        "    text[i] = re.sub(\"NearFatal\",\"Near Fatal\",text[i])\n",
        "    text[i] = re.sub(\"FortWorth\",\"Fort Worth\",text[i])\n",
        "    text[i] = re.sub(\"NowPlaying\",\"Now Playing\",text[i])\n",
        "    text[i] = re.sub(\"OtleyHour\",\"Otley Hour\",text[i])\n",
        "    text[i] = re.sub(\"BayArea\",\"Bay Area\",text[i])\n",
        "    text[i] = re.sub(\"NashvilleTraffic\",\"Nashville Traffic\",text[i])\n",
        "    text[i] = re.sub(\"ElBestia\",\"El Bestia\",text[i])\n",
        "    text[i] = re.sub(\"TruckCrash\",\"Truck Crash\",text[i])\n",
        "    text[i] = re.sub(\"RockyFire\",\"Rocky Fire\", text[i])\n",
        "    text[i] = re.sub(\"CAfire\",\"California fire\",text[i])\n",
        "    text[i] = re.sub(\"personalinjury\",\"personal injury\",text[i])\n",
        "    text[i] = re.sub(\"SantaClara\",\"Santa Clara\",text[i])\n",
        "    text[i] = re.sub(\"ArrestPastorNganga\",\"Arrest\",text[i])\n",
        "    text[i] = re.sub(\"GrowingUpSpoiled\",\"\",text[i])\n",
        "    text[i] = re.sub(\"AberyrtwythShrewsbury\",\"Shrewsbury\",text[i])\n",
        "    text[i] = re.sub(\"WildHorses\",\"Wild Horses\",text[i])\n",
        "    text[i] = re.sub(\"TantoNationalForest\",\"Tanto National Forest\",text[i])\n",
        "    text[i] = re.sub(\"SaltRiverWildHorses\",\"Salt River Wild Horses\",text[i])\n",
        "    text[i] = re.sub(\"ReadingApocalypse\",\"Reading Apocalypse\",text[i])\n",
        "    text[i] = re.sub(\"GeekApocalypse\",\"Geek Apocalypse\",text[i])\n",
        "    text[i] = re.sub(\"DrAyesha\",\"Dr Ayesha\",text[i])\n",
        "    text[i] = re.sub(\"IndiaKoMunTorJawabDoIndian\",\"India\",text[i])\n",
        "    text[i] = re.sub(\"ChicagoArea\",\"Chicago Area\",text[i])\n",
        "    text[i] = re.sub(\"SanFrancisco\",\"San Francisco\",text[i])\n",
        "    text[i] = re.sub(\"NewYork\",\"New York\",text[i])\n",
        "    text[i] = re.sub(\"ColaradoAvalanche\",\"Colarado Avalanche\",text[i])\n",
        "    text[i] = re.sub(\"BioterrorismMass\",\"BioTerrorism Mass\",text[i])\n",
        "    text[i] = re.sub(\"TranscendBlazing\",\"Transcend Blazing\",text[i])\n",
        "    text[i] = re.sub(\"AntiBlight\",\"Anti Blight\",text[i])\n",
        "    text[i] = re.sub(\"RevolutionBlight\",\"Revolution Blight\",text[i])\n",
        "    text[i] = re.sub(\"BellyBombed\",\"Belly Bomb\",text[i])\n",
        "    text[i] = re.sub(\"BanTheBomb\",\"Ban The Bomb\",text[i])\n",
        "    text[i] = re.sub(\"TrainTragedy\",\"Train Tragedy\",text[i])\n",
        "    text[i] = re.sub(\"MultiCasualty\",\"Multi Casualty\",text[i])\n",
        "    text[i] = re.sub(\"PropertyCasualty\",\"Propery Casualty\",text[i])\n",
        "    text[i] = re.sub(\"CollisionNo\",\"Collision\",text[i])\n",
        "    text[i] = re.sub(\"CollisionUnkn\",\"Collision\",text[i])\n",
        "    text[i] = re.sub(\"INCIDENTCrash\",\"Incident Crash\",text[i])\n",
        "    text[i] = re.sub(\"BoltåÊCyclone\",\"Cyclone\",text[i])\n",
        "\n",
        "    remove_digits = str.maketrans('', '', digits)\n",
        "    text[i] = text[i].translate(remove_digits)\n",
        "    text[i] = text[i].translate(str.maketrans('', '', string.punctuation))\n",
        "    text[i] = re.sub(r'\\b\\w{1,2}\\b', '', text[i])\n",
        "    text[i] = [i for i in text[i].lower().split() if i not in stop]\n",
        "    text[i] = \" \".join(text[i])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Swf_eJpyRe3z",
        "colab_type": "code",
        "outputId": "9abf6a1f-c401-4a29-a920-3da610e8b4f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "data.head(4)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>deeds reason earthquake may allah forgive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>forest fire near ronge sask canada</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>residents asked shelter place notified officer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>people receive wildfires evacuation orders cal...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text\n",
              "0   1     NaN      NaN          deeds reason earthquake may allah forgive\n",
              "1   4     NaN      NaN                 forest fire near ronge sask canada\n",
              "2   5     NaN      NaN  residents asked shelter place notified officer...\n",
              "3   6     NaN      NaN  people receive wildfires evacuation orders cal..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcdJ_BK0nDvm",
        "colab_type": "text"
      },
      "source": [
        "# Dealing with missing keyword values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkc0PgztPzGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keywords = list(data[~data.keyword.isna()].keyword.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c2OIln3nYPU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "ed6a6dbb-8129-41ab-b309-6894119f5cc6"
      },
      "source": [
        "list(set(keywords))[:10]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['destroyed',\n",
              " 'body%20bags',\n",
              " 'sirens',\n",
              " 'traumatised',\n",
              " 'rescue',\n",
              " 'sunk',\n",
              " 'battle',\n",
              " 'blew%20up',\n",
              " 'rescuers',\n",
              " 'forest%20fire']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1L-FvqUnRXt",
        "colab_type": "text"
      },
      "source": [
        "Certain keywords had \"%20\" in them. Lets clean them first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7QVHthocpC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(keywords)):\n",
        "    if \"%20\" in keywords[i]:\n",
        "        keywords[i] = keywords[i].split(\"%20\")[0]+\"_\"+keywords[i].split(\"%20\")[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZwDhB2Snjle",
        "colab_type": "text"
      },
      "source": [
        "Now we have proper keywords. (Some with \"_\", but thats okay for now). \n",
        "Lets add them back to the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn7sf1HecqwT",
        "colab_type": "code",
        "outputId": "30069f05-5ec2-4f55-de77-72a3f69b0d3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "data_keyword = data[~data.keyword.isna()]\n",
        "data_wo_keyword = data[data.keyword.isna()]\n",
        "\n",
        "data_keyword.keyword = keywords\n",
        "data = data_keyword.append(data_wo_keyword,ignore_index=True)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:5208: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[name] = value\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjFpB_qwoLup",
        "colab_type": "text"
      },
      "source": [
        "Lets divide the dataset into the collection of samples which have keyword value and collection of samples which has NA keyword value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb7Q80ejcsrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_missing_keyword = data[data.keyword.isna()]\n",
        "data_non_missing_keyword = data[~data.keyword.isna()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIu2xnZ9cuf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "to_find_keyword = data_missing_keyword.text.values.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEeaVWtFodT0",
        "colab_type": "text"
      },
      "source": [
        "Keyword has to exist in tweets, thats the basic idea. We will check corresponding tweets to check if a keyword is present in that particular tweet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvdXGkzWpBdm",
        "colab_type": "text"
      },
      "source": [
        "Basic preprocessing of tweets before we proceed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBzwExn1cwNX",
        "colab_type": "code",
        "outputId": "8ee5b328-73f5-450a-ddcb-276fc81a1334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for i in range(len(to_find_keyword)):\n",
        "    to_find_keyword[i] = to_find_keyword[i].lower()\n",
        "    to_find_keyword[i] = to_find_keyword[i].split(\" \")\n",
        "\n",
        "len(to_find_keyword)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "87"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeIkCB-opFso",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTY_5r7Uc0r_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "set_keywords = set(keywords)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DtXL00-pcBf",
        "colab_type": "text"
      },
      "source": [
        "lets look at the tweets and see if theres a matching keyword present in them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM_6H6wAc3ig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lst = np.ones(87)\n",
        "lst = lst.tolist()\n",
        "# lst = []\n",
        "def fill_keywords():\n",
        "  for i in range(len(to_find_keyword)):\n",
        "      for j in range(len(to_find_keyword[i])):\n",
        "          if to_find_keyword[i][j] in tuple(set_keywords):\n",
        "              lst[i] = to_find_keyword[i][j]\n",
        "          elif j!=len(to_find_keyword[i])-1 and (to_find_keyword[i][j]+\"_\"+to_find_keyword[i][j+1] in tuple(set_keywords)):\n",
        "              lst[i]= to_find_keyword[i][j]+\"_\"+to_find_keyword[i][j+1]\n",
        "\n",
        "fill_keywords()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uz_5ZbM4c5Hg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "non_related = [i for i in range(len(lst)) if lst[i]==1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFwDq9XEp7LP",
        "colab_type": "text"
      },
      "source": [
        "These are the tweets which do not have any existing keywords present in them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjJ5W-11c7yk",
        "colab_type": "code",
        "outputId": "878db217-c617-48d4-9521-674b6514c152",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "for i in non_related:\n",
        "    print(i,to_find_keyword[i])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15 ['man']\n",
            "16 ['love', 'fruits']\n",
            "17 ['summer', 'lovely']\n",
            "18 ['car', 'fast']\n",
            "19 ['goal']\n",
            "20 ['ridiculous']\n",
            "21 ['london', 'cool']\n",
            "22 ['love', 'skiing']\n",
            "23 ['wonderful', 'day']\n",
            "24 ['looooool']\n",
            "25 ['way', 'eat', 'shit']\n",
            "26 ['nyc', 'last', 'week']\n",
            "27 ['love', 'girlfriend']\n",
            "28 ['cooool']\n",
            "29 ['like', 'pasta']\n",
            "30 ['end']\n",
            "67 ['probably', 'still', 'show', 'life', 'arsenal', 'yesterday']\n",
            "68 ['hey']\n",
            "69 ['nice', 'hat']\n",
            "70 ['fuck']\n",
            "71 ['like', 'cold']\n",
            "72 ['']\n",
            "73 ['tell']\n",
            "74 ['']\n",
            "75 ['awesome']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xpr8Bf1SqB6d",
        "colab_type": "text"
      },
      "source": [
        "Lets create a new keyword, \"random\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PWh9gVVc-nd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(lst)):\n",
        "    if lst[i] == 1:\n",
        "        lst[i] = \"random\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sk-VhWKSqHmQ",
        "colab_type": "text"
      },
      "source": [
        "Combine the two datasets back to one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2dpWkdQdDs_",
        "colab_type": "code",
        "outputId": "40b587ee-3a5a-4f9e-bf80-24155fc4f160",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "data_missing_keyword.keyword = lst\n",
        "data = data_missing_keyword.append(data_non_missing_keyword,ignore_index=True)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:5208: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[name] = value\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKBow96TqR6Y",
        "colab_type": "text"
      },
      "source": [
        "Converting string into list of words. For ease during the next phase of data engineering. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzxuYrSodGOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = data.text.values\n",
        "for i in range(len(text)):    \n",
        "    text[i] = [i for i in text[i].lower().split()]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72ishyNUdMMY",
        "colab_type": "code",
        "outputId": "2d8ca9ff-a451-4ab3-b5bb-629b41e6b9ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "data.head(4)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>earthquake</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[deeds, reason, earthquake, may, allah, forgive]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>fire</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[forest, fire, near, ronge, sask, canada]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>evacuation</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[residents, asked, shelter, place, notified, o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>evacuation</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[people, receive, wildfires, evacuation, order...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id     keyword location                                               text\n",
              "0   1  earthquake      NaN   [deeds, reason, earthquake, may, allah, forgive]\n",
              "1   4        fire      NaN          [forest, fire, near, ronge, sask, canada]\n",
              "2   5  evacuation      NaN  [residents, asked, shelter, place, notified, o...\n",
              "3   6  evacuation      NaN  [people, receive, wildfires, evacuation, order..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rNGJaCpqmCN",
        "colab_type": "text"
      },
      "source": [
        "# Dealing with Location data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cExzM2Vde6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_location= data[~data.location.isna()]\n",
        "data_location_na= data[data.location.isna()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMie_FCydk5m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lc = data_location.location.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKOpJhAwqwpI",
        "colab_type": "text"
      },
      "source": [
        "Lets use a python package called \"geotext\". This package easily finds out if a string has city or country names in it.\n",
        "\n",
        "\n",
        "\n",
        "1.   But the requirement of geotext is that it needs city/country names to begin with a capital letter. Hence, small transformation of data is needed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzLRi3SEdmT1",
        "colab_type": "code",
        "outputId": "56531665-c73b-4e4c-98d4-31a21e4e6586",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "for i in range(len(lc)):\n",
        "    lc[i] = \" \".join(w.capitalize() for w in lc[i].split())\n",
        "lc"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Birmingham', 'Est. September 2012 - Bristol', 'Africa', ...,\n",
              "       'Acey Mountain Islanddåçtorontoåè', 'Los Angeles',\n",
              "       'Brussels, Belgium'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRdX7FZsrRiI",
        "colab_type": "text"
      },
      "source": [
        "As we can see, location values are to be cleaned too. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtZ80itBdo_X",
        "colab_type": "code",
        "outputId": "daf230fc-57e2-4543-b51b-be17f56c8068",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!pip install geotext"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting geotext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/c5/36351193092cb4c1d7002d2a3babe5e72ae377868473933d6f63b41e5454/geotext-0.4.0-py2.py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 3.4MB/s \n",
            "\u001b[?25hInstalling collected packages: geotext\n",
            "Successfully installed geotext-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfYtsQ93reMC",
        "colab_type": "text"
      },
      "source": [
        "First lets see if the string has a city name in it. If not found, lets search for a country name. If that fails too, lets use \"unknown\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH3XfFrrdriO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from geotext import GeoText\n",
        "\n",
        "\n",
        "lstk = []\n",
        "for i in range(len(lc)):\n",
        "\n",
        "    places = GeoText(lc[i])\n",
        "    if len(places.cities)>=1:\n",
        "        lstk.append(places.cities)\n",
        "    elif len(places.countries)>=1:\n",
        "        lstk.append(places.countries)\n",
        "    else:\n",
        "        lstk.append(\"unknown\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNsiz2pZdtfF",
        "colab_type": "code",
        "outputId": "cf92ec6b-12f5-410c-911d-58e794fd8aaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "lstk[:10]"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Birmingham'],\n",
              " ['Bristol'],\n",
              " 'unknown',\n",
              " ['Philadelphia'],\n",
              " ['London'],\n",
              " ['Pretoria'],\n",
              " 'unknown',\n",
              " 'unknown',\n",
              " 'unknown',\n",
              " 'unknown']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srKNPV2Vdvk0",
        "colab_type": "code",
        "outputId": "86173497-5f2e-4df8-b9e3-c0b32eb79084",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(lc) == len(lstk)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55xT73N6r8xu",
        "colab_type": "text"
      },
      "source": [
        "Combination of strings and lists, Doesn't look so good :P"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX49VZ9EdzOQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(lstk)):\n",
        "    if type(lstk[i])==list:\n",
        "        lstk[i] = ' '.join(lstk[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q919kzX4sGE1",
        "colab_type": "text"
      },
      "source": [
        "Location also has some values such as \"of\" and \"man\". Lets remove those."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6diQCJRd5tJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(lstk)):\n",
        "    lstk[i] = re.sub(r'\\W+', ' ', lstk[i])\n",
        "    lstk[i] = lstk[i].lower()\n",
        "    if lstk[i] == \"of\" or lstk[i] == \"man\":\n",
        "        lstk[i] = \"unknown\"   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ECbpNYDd7yP",
        "colab_type": "code",
        "outputId": "9ff7b0cd-0fa8-41d9-a981-bdd0dc16ca43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "data_location.location= lstk\n",
        "data = data_location.append(data_location_na,ignore_index=True)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:5208: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[name] = value\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ioi5VrgGd-mg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstk_unique = list(set(lstk))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSAMJBVQsR8F",
        "colab_type": "text"
      },
      "source": [
        "Now we have cleaned our location column of NON-NA location samples. Next step is to impute locations for NA samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLol9iuHeAXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data1 = data[data.location.isna()]\n",
        "data2 = data[~data.location.isna()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhKmSpNTeBzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "allNotExistingCorpus = data1.text.values.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNRxlQkIsjg2",
        "colab_type": "text"
      },
      "source": [
        "Lets see if a particular location exists in tweet. otherwise, there is noway we can infer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kGCb-1WeF3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arr1 = [\"unknown\" for i in range(len(data1))]\n",
        "for i in range(len(arr1)):\n",
        "    for j in range(len(lstk_unique)):\n",
        "        if lstk_unique[j] in allNotExistingCorpus[i] and i<len(arr1):\n",
        "            arr1[i] = lstk_unique[j]\n",
        "        elif \"nyc\" in allNotExistingCorpus[i]:\n",
        "            arr1[i] = \"new york\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvGPJTE3eILW",
        "colab_type": "code",
        "outputId": "58a190d6-7764-4f37-e1b0-b25f6da5a9ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "arr1[:10]"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['unknown',\n",
              " 'canada',\n",
              " 'unknown',\n",
              " 'unknown',\n",
              " 'unknown',\n",
              " 'unknown',\n",
              " 'colorado',\n",
              " 'unknown',\n",
              " 'unknown',\n",
              " 'unknown']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOWwFiq5eKfw",
        "colab_type": "code",
        "outputId": "91ae8f59-3b6d-4dae-f952-ab04b9e1a7e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "data1.location = arr1\n",
        "data = data1.append(data2,ignore_index=True)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py:5208: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[name] = value\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2HRwEEstJmy",
        "colab_type": "text"
      },
      "source": [
        "We have, to a certain extent, cleaned the data. Lets transform data to the required format for our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS9yvOHqtYT-",
        "colab_type": "text"
      },
      "source": [
        "Our y has both targets and ids. Lets use that to separate our main dataset, Also lets sort according to id since we have separated and appeneded multiple times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlxb7I0geNwM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = data[data.id.isin(y.id)]\n",
        "test = data[~data.id.isin(y.id)]\n",
        "\n",
        "train = train.sort_values('id')\n",
        "test = test.sort_values('id')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GfqoEZutlwQ",
        "colab_type": "text"
      },
      "source": [
        "Lets get the required arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNmtnFq9eTK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keyword = train.keyword.values\n",
        "location = train.location.values\n",
        "text_ = train.text.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0BQ_2kttodV",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgouT5b6vckm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sAfGcZ0vdyJ",
        "colab_type": "text"
      },
      "source": [
        "To tokenize the three arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YUWlg3htql4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def tokenize(num, list_arr):\n",
        "  num = num #max size of library\n",
        "  name_tokenizer = Tokenizer(num_words=num)\n",
        "  name_tokenizer.fit_on_texts(list(list_arr))\n",
        "  name_tokenized_list = name_tokenizer.texts_to_sequences(list(list_arr))\n",
        "\n",
        "  #if the library ends up smaller then the max size, update the info\n",
        "  if len(name_tokenizer.word_index) < num:\n",
        "      num = len(name_tokenizer.word_index)\n",
        "      \n",
        "  print('Number of words:', num)\n",
        "  pad_name = len(max(name_tokenized_list,key=len))\n",
        "  final_array = pad_sequences(name_tokenized_list, maxlen=pad_name, padding='post')\n",
        "\n",
        "  return num, name_tokenizer, name_tokenized_list, pad_name, final_array\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBo9EFiRvkt6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ec2d3c13-d7c9-4ac8-d87f-dfcf52f271e0"
      },
      "source": [
        "max_num_word_text, tokenizer_text, list_tokenized_text, pad_max_text, text_array = tokenize(100000, text_)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words: 14016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqsZ_1p3vyR-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f6ca2da7-635d-4eec-ddbc-9dfed6c9a9d6"
      },
      "source": [
        "max_num_word_keyword, tokenizer_keyword, list_tokenized_keyword, pad_max_keyword, keyword_array = tokenize(100000, keyword)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words: 230\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwFyKgLUv_l9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8e7f3c31-e1ba-447d-eb52-4980ad5aab2f"
      },
      "source": [
        "max_num_word_location, tokenizer_location, list_tokenized_location, pad_max_location, location_array = tokenize(100000, location)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words: 857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdDNHvuCwZlc",
        "colab_type": "text"
      },
      "source": [
        "One hot encoding target values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeHiPC_YegWm",
        "colab_type": "code",
        "outputId": "733b4c70-d66b-413f-95d6-96d8c7e2f0a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.utils import np_utils\n",
        "print(pad_max_keyword,pad_max_location,pad_max_text)\n",
        "y = y.target.values\n",
        "y=np_utils.to_categorical(y,2)\n"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 4 21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACJK3174wkil",
        "colab_type": "text"
      },
      "source": [
        "A simple LSTM model along with separate embeddings for the three arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGisrcuZejtc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Concatenate, Dense, Dropout, Flatten, LSTM\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Reshape, Dot\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Add, Activation, Lambda, Conv1D, MaxPool1D\n",
        "from keras.layers import Bidirectional, SpatialDropout1D\n",
        "\n",
        "def disaster_or_not():\n",
        "\n",
        "    key = Input(shape=(pad_max_keyword,))\n",
        "    k = Embedding(max_num_word_keyword+1, 50)(key)\n",
        "    k = Flatten()(k)\n",
        "\n",
        "    location = Input(shape=(pad_max_location,))\n",
        "    l = Embedding(max_num_word_location+1, 50)(location)\n",
        "    l = Flatten()(l)\n",
        "\n",
        "    tweet = Input(shape=(pad_max_text,))\n",
        "    t = Embedding(max_num_word_text+1, 50)(tweet)\n",
        "    t = Flatten()(t)\n",
        "\n",
        "    x = Concatenate()([k, l, t])\n",
        "    x = Reshape((1,1350))(x)\n",
        "    x = SpatialDropout1D(0.4)(x)\n",
        "    x = LSTM(256, dropout=0.2, recurrent_dropout=0.2) (x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Dense(2, activation=\"softmax\")(x)\n",
        "    \n",
        "    model = Model(inputs=[key, location, tweet], outputs=x)\n",
        "    opt = Adam(lr=0.001)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNMfHbm1epva",
        "colab_type": "code",
        "outputId": "98330b37-e4d8-45d6-b903-602d926fc774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        }
      },
      "source": [
        "model = disaster_or_not()\n",
        "model.summary()"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            (None, 2)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_8 (InputLayer)            (None, 4)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_9 (InputLayer)            (None, 21)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_7 (Embedding)         (None, 2, 50)        11550       input_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_8 (Embedding)         (None, 4, 50)        42900       input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_9 (Embedding)         (None, 21, 50)       700850      input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_7 (Flatten)             (None, 100)          0           embedding_7[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_8 (Flatten)             (None, 200)          0           embedding_8[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "flatten_9 (Flatten)             (None, 1050)         0           embedding_9[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 1350)         0           flatten_7[0][0]                  \n",
            "                                                                 flatten_8[0][0]                  \n",
            "                                                                 flatten_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "reshape_3 (Reshape)             (None, 1, 1350)      0           concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_3 (SpatialDro (None, 1, 1350)      0           reshape_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   (None, 256)          1645568     spatial_dropout1d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 256)          0           lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 2)            514         activation_3[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 2,401,382\n",
            "Trainable params: 2,401,382\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxs7YM2gxO3w",
        "colab_type": "text"
      },
      "source": [
        "One epoch seems to work the best. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YBGNou7esd2",
        "colab_type": "code",
        "outputId": "2c47c39a-6823-4b48-80b8-0df2cc370747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "history = model.fit(x=[keyword_array, location_array, text_array], y=y, batch_size=32, epochs=1,verbose=1)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "7613/7613 [==============================] - 3s 349us/step - loss: 0.3175 - acc: 0.8725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXyEbj99xSpu",
        "colab_type": "text"
      },
      "source": [
        "To predict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wNHDKM-evZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keyword_test = test.keyword.values\n",
        "location_test = test.location.values\n",
        "text_test = test.text.values\n",
        "\n",
        "test_tok = tokenizer_text.texts_to_sequences(list(text_test))\n",
        "text_array_test = pad_sequences(test_tok, maxlen=pad_max_text, padding='post')\n",
        "\n",
        "test_key = tokenizer_keyword.texts_to_sequences(list(keyword_test))\n",
        "key_array_test = pad_sequences(test_key, maxlen=pad_max_keyword, padding='post')\n",
        "\n",
        "test_loc = tokenizer_location.texts_to_sequences(list(location_test))\n",
        "loc_array_test = pad_sequences(test_loc, maxlen=pad_max_location, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izgzngpke97W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "preds = model.predict([key_array_test, loc_array_test, text_array_test])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDd4oycZfBtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds_final = []\n",
        "for i in preds:\n",
        "  preds_final.append(i.argmax())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Utbl8cUfH0k",
        "colab_type": "code",
        "outputId": "bae81c0c-58a3-44ec-82c2-809036ba0c4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "preds_final[:10]"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9ulNzFGfJq0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}